"""Auto-generated tests for commit 257fe908: add fsdp assert back

Tests the changes introduced in branch auto/20260202221742 relative to main.
Specifically tests the new prefix-matching ignore rule in quantize_params_compressed_tensors.
Generated by /auto-ci skill.
"""

import pytest
import torch

from miles.backends.megatron_utils.megatron_to_hf.processors.quantizer_compressed_tensors import (
    quantize_params_compressed_tensors,
)


def _make_quantization_config(ignore_rules=None):
    """Helper to create a minimal quantization config for testing."""
    return {
        "config_groups": {
            "group_0": {
                "weights": {
                    "group_size": 128,
                    "symmetric": True,
                },
            },
        },
        "ignore": ignore_rules or [],
    }


def _make_weight(name, rows=256, cols=256):
    """Create a named weight tensor that would normally be quantized."""
    return (name, torch.randn(rows, cols))


class TestIgnoreRulePrefixMatching:
    """Tests for the new prefix-matching ignore rule (name.startswith(r))."""

    def test_exact_match_still_works(self):
        """Exact name match should still ignore the param."""
        params = [_make_weight("model.layer.weight")]
        config = _make_quantization_config(ignore_rules=["model.layer.weight"])
        results = quantize_params_compressed_tensors(params, config)
        # Ignored param is returned as-is (single tuple, not expanded to scale/shape)
        assert len(results) == 1
        assert results[0][0] == "model.layer.weight"

    def test_regex_match_still_works(self):
        """Regex rules (re: prefix) should still work."""
        params = [_make_weight("model.embed_tokens.weight")]
        config = _make_quantization_config(ignore_rules=["re:.*embed.*"])
        results = quantize_params_compressed_tensors(params, config)
        assert len(results) == 1
        assert results[0][0] == "model.embed_tokens.weight"

    def test_prefix_match_ignores_param(self):
        """NEW: A prefix rule should ignore params whose names start with the rule."""
        params = [_make_weight("model.layers.0.self_attn.q_proj.weight")]
        config = _make_quantization_config(ignore_rules=["model.layers.0.self_attn"])
        results = quantize_params_compressed_tensors(params, config)
        # Should be ignored (returned as-is, 1 tuple)
        assert len(results) == 1
        assert results[0][0] == "model.layers.0.self_attn.q_proj.weight"

    def test_prefix_match_multiple_params(self):
        """Prefix rule should ignore all params matching the prefix."""
        params = [
            _make_weight("model.layers.0.self_attn.q_proj.weight"),
            _make_weight("model.layers.0.self_attn.k_proj.weight"),
            _make_weight("model.layers.0.mlp.gate_proj.weight"),
        ]
        config = _make_quantization_config(ignore_rules=["model.layers.0.self_attn"])
        results = quantize_params_compressed_tensors(params, config)

        result_names = [r[0] for r in results]
        # The two self_attn params should be returned as-is (ignored)
        assert "model.layers.0.self_attn.q_proj.weight" in result_names
        assert "model.layers.0.self_attn.k_proj.weight" in result_names
        # The mlp param should be quantized (expanded to multiple tuples: weight_packed, scale, shape)
        assert "model.layers.0.mlp.gate_proj.weight_packed" in result_names

    def test_non_matching_prefix_does_not_ignore(self):
        """A prefix that doesn't match should allow quantization."""
        params = [_make_weight("model.layers.1.mlp.gate_proj.weight")]
        config = _make_quantization_config(ignore_rules=["model.layers.0"])
        results = quantize_params_compressed_tensors(params, config)
        # Should be quantized (expanded to weight_packed, scale, shape)
        result_names = [r[0] for r in results]
        assert "model.layers.1.mlp.gate_proj.weight_packed" in result_names

    def test_empty_ignore_rules(self):
        """No ignore rules means everything gets quantized."""
        params = [_make_weight("model.layer.weight")]
        config = _make_quantization_config(ignore_rules=[])
        results = quantize_params_compressed_tensors(params, config)
        result_names = [r[0] for r in results]
        assert "model.layer.weight_packed" in result_names

    def test_non_weight_params_always_skipped(self):
        """Params not ending in .weight should always be returned as-is."""
        params = [("model.layer.bias", torch.randn(256))]
        config = _make_quantization_config(ignore_rules=[])
        results = quantize_params_compressed_tensors(params, config)
        assert len(results) == 1
        assert results[0][0] == "model.layer.bias"

    def test_1d_weight_always_skipped(self):
        """1D weight tensors (dim < 2) should always be returned as-is."""
        params = [("model.norm.weight", torch.randn(256))]
        config = _make_quantization_config(ignore_rules=[])
        results = quantize_params_compressed_tensors(params, config)
        assert len(results) == 1
        assert results[0][0] == "model.norm.weight"

    @pytest.mark.parametrize(
        "rule,name,should_ignore",
        [
            ("model.layers.0", "model.layers.0.attn.weight", True),
            ("model.layers.0", "model.layers.01.attn.weight", True),  # prefix match is literal
            ("model.layers.", "model.layers.5.attn.weight", True),
            ("model.embed", "model.embed_tokens.weight", True),
            ("model.layers.1", "model.layers.0.attn.weight", False),
            ("other.prefix", "model.layers.0.attn.weight", False),
        ],
    )
    def test_prefix_match_parametrized(self, rule, name, should_ignore):
        """Parametrized test for various prefix matching scenarios."""
        params = [_make_weight(name)]
        config = _make_quantization_config(ignore_rules=[rule])
        results = quantize_params_compressed_tensors(params, config)
        result_names = [r[0] for r in results]
        if should_ignore:
            # Returned as-is: only the original name
            assert name in result_names
            assert f"{name}_packed" not in result_names
        else:
            # Quantized: name replaced with name_packed
            name_base = name.replace(".weight", "")
            assert f"{name_base}.weight_packed" in result_names
